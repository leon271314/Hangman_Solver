{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "o65X908G8kLW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "\n",
        "\n",
        "class generic_model(nn.Module):\n",
        "    \"\"\"\n",
        "    contains basic functions for storing and loading a model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "\n",
        "        super(generic_model, self).__init__()\n",
        "\n",
        "        self.config_file = config\n",
        "\n",
        "    def loss(self, predicted, truth):\n",
        "\n",
        "        return self.loss_func(predicted, truth)\n",
        "    \n",
        "    def save_model(self, is_best, epoch, train_loss, test_loss, rnn_name, layers, hidden_dim):\n",
        "\n",
        "        base_path = self.config_file['models']\n",
        "        if is_best:\n",
        "            filename = os.path.join(base_path, 'best_' + '_'.join([rnn_name, str(layers), str(hidden_dim)]) + '.pth')\n",
        "        else:\n",
        "            filename = os.path.join(base_path, str(epoch) + '_' + '_'.join([rnn_name, str(layers), str(hidden_dim)]) + '.pth')\n",
        "\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'train_loss': train_loss,\n",
        "            'test_loss': test_loss,\n",
        "        }, filename)\n",
        "\n",
        "        print(\"Saved model\")\n",
        "\n",
        "    def load_model(self, mode, rnn_name, layers, hidden_dim, epoch=None):\n",
        "\n",
        "        if mode == 'test' or mode == 'test_one':\n",
        "\n",
        "            try:\n",
        "                if epoch is None:\n",
        "                    filename = os.path.join(self.config_file['models'], 'best_' + '_'.join(\n",
        "                        [rnn_name, str(layers), str(hidden_dim)]) + '.pth')\n",
        "                else:\n",
        "                    filename = os.path.join(self.config_file['models'], str(epoch) + '_' + '_'.join(\n",
        "                        [rnn_name, str(layers), str(hidden_dim)]) + '.pth')\n",
        "                print(filename)\n",
        "\n",
        "                checkpoint = torch.load(filename, map_location=lambda storage, loc: storage)\n",
        "                # load model parameters\n",
        "                self.load_state_dict(checkpoint['model_state_dict'])\n",
        "                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "                print(\"Loaded pretrained model from:\", filename)\n",
        "\n",
        "            except:\n",
        "                print(\"Couldn't find model for testing\")\n",
        "                exit(0)\n",
        "\n",
        "        else:\n",
        "      \n",
        "            if epoch is not None:\n",
        "                filename = self.config_file['models'] + str(epoch) + '_' + '_'.join(\n",
        "                    [rnn_name, str(layers), str(hidden_dim)]) + '.pth'\n",
        "            else:\n",
        "                directory = [x.split('_') for x in os.listdir(self.config_file['models'])]\n",
        "                to_check = []\n",
        "                for poss in directory:\n",
        "                    try:\n",
        "                        to_check.append(int(poss[0]))\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "                if len(to_check) == 0:\n",
        "                    print(\"No pretrained model found\")\n",
        "                    return 0, [], []\n",
        "       \n",
        "                filename = os.path.join(self.config_file['models'], str(max(to_check)) + '_' + '_'.join(\n",
        "                    [rnn_name, str(layers), str(hidden_dim)]) + '.pth')\n",
        "\n",
        "\n",
        "            checkpoint = torch.load(filename, map_location=lambda storage, loc: storage)\n",
        "            self.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "            print(\"Loaded pretrained model from:\", filename)\n",
        "\n",
        "            return checkpoint['epoch'], checkpoint['train_loss'], checkpoint['test_loss']\n",
        "\n",
        "\n",
        "class RNN(generic_model):\n",
        "\n",
        "    def __init__(self, config):\n",
        "\n",
        "        super(RNN, self).__init__(config)\n",
        "\n",
        "\n",
        "        self.rnn_name = config['rnn']\n",
        "        self.input_dim = config['vocab_size'] + 1\n",
        "        self.hidden_dim = config['hidden_dim']\n",
        "        self.num_layers = config['num_layers']\n",
        "        self.embed_dim = config['embedding_dim']\n",
        "        self.output_dim = config['vocab_size']\n",
        "\n",
        "        if config['use_embedding']:\n",
        "            self.use_embedding = True\n",
        "            self.embedding = nn.Embedding(self.input_dim, self.embed_dim)\n",
        "        else:\n",
        "            self.use_embedding = False\n",
        "\n",
        " \n",
        "        if self.rnn_name == 'Transformer':\n",
        "            in_features = self.embed_dim +config['miss_linear_dim']\n",
        "        else:\n",
        "            in_features = config['miss_linear_dim'] + self.hidden_dim*2\n",
        "        mid_features = config['output_mid_features']\n",
        "        self.linear1_out = nn.Linear(in_features, mid_features)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2_out = nn.Linear(mid_features, self.output_dim)\n",
        "\n",
        "\n",
        "        self.miss_linear = nn.Linear(config['vocab_size'], config['miss_linear_dim'])\n",
        "\n",
        "        if self.rnn_name == 'LSTM':\n",
        "            self.encoder = nn.LSTM(input_size=self.embed_dim if self.use_embedding else self.input_dim, hidden_size=self.hidden_dim, num_layers=self.num_layers,\n",
        "                               dropout=config['dropout'],\n",
        "                               bidirectional=True, batch_first=True)\n",
        "        elif self.rnn_name == 'GRU':\n",
        "            self.encoder = nn.GRU(input_size=self.embed_dim if self.use_embedding else self.input_dim, hidden_size=self.hidden_dim, num_layers=self.num_layers,\n",
        "                              dropout=config['dropout'],\n",
        "                              bidirectional=True, batch_first=True)\n",
        "        elif self.rnn_name == 'Transformer':\n",
        "            self.cls_token = nn.Parameter(torch.randn(1, 1, self.embed_dim), requires_grad=True)\n",
        "            encoder_layer = nn.TransformerEncoderLayer(d_model=self.embed_dim, nhead=8, dropout=config['dropout'], batch_first=True)\n",
        "            self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=self.num_layers)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=config['lr'])\n",
        "\n",
        "    def forward(self, x, x_lens, miss_chars):\n",
        "        \"\"\"\n",
        "        Forward pass through RNN\n",
        "        :param x: input tensor of shape (batch size, max sequence length, input_dim)\n",
        "        :param x_lens: actual lengths of each sequence < max sequence length (since padded with zeros)\n",
        "        :param miss_chars: tensor of length batch_size x vocab size. 1 at index i indicates that ith character is NOT present\n",
        "        :return: tensor of shape (batch size, max sequence length, output dim)\n",
        "        \"\"\"\n",
        "        if self.use_embedding:\n",
        "            x = self.embedding(x)\n",
        "\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        if self.rnn_name != 'Transformer':\n",
        "            x = torch.nn.utils.rnn.pack_padded_sequence(x, x_lens.cpu(), batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        if self.rnn_name == 'LSTM':\n",
        "            output, (hidden, _) = self.encoder(x)\n",
        "        elif self.rnn_name == 'GRU':\n",
        "            output, hidden = self.encoder(x)\n",
        "        elif self.rnn_name == 'Transformer':\n",
        "            x = torch.cat((self.cls_token.repeat(batch_size, 1, 1), x), dim=1)\n",
        "            hidden = self.encoder(x)\n",
        "            hidden = hidden[:, 0, :]\n",
        "\n",
        "        if self.rnn_name != 'Transformer':\n",
        "            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_dim)\n",
        "            hidden = hidden[-1]\n",
        "            hidden = hidden.permute(1, 0, 2)\n",
        "            hidden = hidden.contiguous().view(hidden.shape[0], -1)\n",
        "\n",
        "\n",
        "        miss_chars = self.miss_linear(miss_chars)\n",
        "\n",
        "        concatenated = torch.cat((hidden, miss_chars), dim=1)\n",
        "    \n",
        "        return self.linear2_out(self.relu(self.linear1_out(concatenated)))\n",
        "\n",
        "    def calculate_loss(self, model_out, labels, input_lens, miss_chars, use_cuda):\n",
        "        \"\"\"\n",
        "        :param model_out: tensor of shape (batch size, max sequence length, output dim) from forward pass\n",
        "        :param labels: tensor of shape (batch size, vocab_size). 1 at index i indicates that ith character should be predicted\n",
        "        :param: miss_chars: tensor of length batch_size x vocab size. 1 at index i indicates that ith character is NOT present\n",
        "                            passed here to check if model's output probability of missed_chars is decreasing\n",
        "        \"\"\"\n",
        "        outputs = nn.functional.log_softmax(model_out, dim=1)\n",
        "\n",
        "        miss_penalty = torch.sum(outputs*miss_chars, dim=(0,1))/outputs.shape[0]\n",
        "\n",
        "        input_lens = input_lens.float()\n",
        "  \n",
        "        weights_orig = (1/input_lens)/torch.sum(1/input_lens).unsqueeze(-1)\n",
        "        weights = torch.zeros((weights_orig.shape[0], 1))\n",
        "\n",
        "        weights[:, 0] = weights_orig\n",
        "\n",
        "        if use_cuda:\n",
        "            weights = weights.cuda()\n",
        "\n",
        "        loss_func = nn.BCEWithLogitsLoss(weight=weights, reduction='sum')\n",
        "        actual_penalty = loss_func(model_out, labels)\n",
        "        return actual_penalty, miss_penalty\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "1Jdngf_V8kLY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import yaml\n",
        "\n",
        "np.random.seed(7)\n",
        "\n",
        "extra_vocab = 1\n",
        "\n",
        "def filter_and_encode(word, vocab_size, min_len, char_to_id):\n",
        "    \"\"\"\n",
        "    checks if word length is greater than threshold and returns one-hot encoded array along with character sets\n",
        "    :param word: word string\n",
        "    :param vocab_size: size of vocabulary (26 in this case)\n",
        "    :param min_len: word with length less than this is not added to the dataset\n",
        "    :param char_to_id\n",
        "    \"\"\"\n",
        "\n",
        "    word = word.strip().lower()\n",
        "    if len(word) < min_len:\n",
        "        return None, None, None\n",
        "\n",
        "    encoding = np.zeros((len(word), vocab_size + extra_vocab))\n",
        "\n",
        "    chars = {k: [] for k in range(vocab_size)}\n",
        "\n",
        "    for i, c in enumerate(word):\n",
        "        idx = char_to_id[c]\n",
        "\n",
        "        chars[idx].append(i)\n",
        "\n",
        "        encoding[i][idx] = 1\n",
        "\n",
        "    return encoding, [x for x in chars.values() if len(x)], set(list(word))\n",
        "\n",
        "\n",
        "def batchify_words(batch, vocab_size, using_embedding):\n",
        "    \"\"\"\n",
        "    converts a list of words into a batch by padding them to a fixed length array\n",
        "    :param batch: a list of words encoded using filter_and_encode function\n",
        "    :param: size of vocabulary (26 in our case)\n",
        "    :param: use_embedding: if True,\n",
        "    \"\"\"\n",
        "\n",
        "    total_seq = len(batch)\n",
        "    if using_embedding:\n",
        "\n",
        "        max_len = max([len(x) for x in batch])\n",
        "        final_batch = []\n",
        "\n",
        "        for word in batch:\n",
        "            if max_len != len(word):\n",
        "   \n",
        "                zero_vec = vocab_size*np.ones((max_len - word.shape[0]))\n",
        "                word = np.concatenate((word, zero_vec), axis=0)\n",
        "            final_batch.append(word)\n",
        "\n",
        "        return np.array(final_batch)\n",
        "    else:\n",
        "        max_len = max([x.shape[0] for x in batch])\n",
        "        final_batch = []\n",
        "\n",
        "        for word in batch:\n",
        "\n",
        "            if max_len != word.shape[0]:\n",
        "                zero_vec = np.zeros((max_len - word.shape[0], vocab_size + extra_vocab))\n",
        "                word = np.concatenate((word, zero_vec), axis=0)\n",
        "            final_batch.append(word)\n",
        "\n",
        "        return np.array(final_batch)\n",
        "\n",
        "\n",
        "def encoded_to_string(encoded, target, missed, encoded_len, char_to_id, use_embedding):\n",
        "    \"\"\"\n",
        "    convert an encoded input-output pair back into a string so that we can observe the input into the model\n",
        "    encoded: array of dimensions padded_word_length x vocab_size\n",
        "    target: 1 x vocab_size array with 1s at indices wherever character is present\n",
        "    missed: 1 x vocav_size array with 1s at indices wherever a character which is NOT in the word, is present\n",
        "    encoded_len: length of word. Needed to retrieve the original word from the padded word\n",
        "    char_to_id: dict which maps characters to ids\n",
        "    use_embedding: if character embeddings are used\n",
        "    \"\"\"\n",
        "\n",
        "    id_to_char = {v:k for k, v in char_to_id.items()}\n",
        "\n",
        "    if use_embedding:\n",
        "        word = [id_to_char[x] if x < len(char_to_id) - 1 else '*' for x in list(encoded[:encoded_len])]\n",
        "    else:\n",
        "        word = [id_to_char[x] if x < len(char_to_id) - 1 else '*' for x in list(np.argmax(encoded[:encoded_len, :], axis=1))]\n",
        "\n",
        "    word = ''.join(word)\n",
        "    target = [id_to_char[x] for x in list(np.where(target != 0)[0])]\n",
        "    missed = [id_to_char[x] for x in list(np.where(missed != 0)[0])]\n",
        "    print(\"Word, target and missed characters:\", word, target, missed)\n",
        "\n",
        "\n",
        "\n",
        "class WordDataset(Dataset):\n",
        "    def __init__(self, mode, config):\n",
        "        self.mode = mode\n",
        "        self.vocab_size = config['vocab_size']\n",
        "        self.blank_vec = np.zeros((1, self.vocab_size + extra_vocab))\n",
        "        self.blank_vec[0, self.vocab_size] = 1\n",
        "        self.cur_epoch = 0\n",
        "        self.total_epochs = config['epochs']\n",
        "\n",
        "        self.char_to_id = {chr(97+x): x for x in range(self.vocab_size)}\n",
        "        self.char_to_id['BLANK'] = self.vocab_size\n",
        "        self.id_to_char = {v:k for k, v in self.char_to_id.items()}\n",
        "\n",
        "        self.drop_uniform = config['drop_uniform']\n",
        "        self.use_embedding = config['use_embedding']\n",
        "        self.min_len = config['min_len']\n",
        "\n",
        "        if mode == 'train':\n",
        "            filename = config['dataset'] + \"train_set_90.txt\"\n",
        "        else:\n",
        "            filename = config['dataset'] + \"test_set_10.txt\"\n",
        "\n",
        "        pkl_path = config['pickle'] + mode + '_input_dump.pkl'\n",
        "        if os.path.exists(pkl_path):\n",
        "            with open(pkl_path, 'rb') as f:\n",
        "                self.final_encoded = pickle.load(f)\n",
        "        else:\n",
        "            corpus = []\n",
        "            with open(filename, 'r') as f:\n",
        "                corpus = f.readlines()\n",
        "\n",
        "            self.final_encoded = []\n",
        "            for i, word in enumerate(corpus):\n",
        "                encoding, unique_pos, chars = filter_and_encode(word, self.vocab_size, self.min_len, self.char_to_id)\n",
        "                if encoding is not None:\n",
        "                    self.final_encoded.append((encoding, unique_pos, chars))\n",
        "\n",
        "            with open(pkl_path, 'wb') as f:\n",
        "                pickle.dump(self.final_encoded, f)\n",
        "\n",
        "        print(\"Length of \" + mode + \" dataset:\", len(self.final_encoded))\n",
        "\n",
        "    def update_epoch(self, epoch):\n",
        "        self.cur_epoch = epoch\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.final_encoded)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        word, unique_pos, chars = self.final_encoded[idx]\n",
        "\n",
        "        all_chars = list(self.char_to_id.keys())\n",
        "        all_chars.remove('BLANK')\n",
        "        all_chars = set(all_chars)\n",
        "\n",
        "        drop_prob = 1/(1+np.exp(-self.cur_epoch/self.total_epochs))\n",
        "        num_to_drop = np.random.binomial(len(unique_pos), drop_prob)\n",
        "        if num_to_drop == 0:\n",
        "            num_to_drop = 1\n",
        "\n",
        "        if self.drop_uniform:\n",
        "            to_drop = np.random.choice(len(unique_pos), num_to_drop, replace=False)\n",
        "        else:\n",
        "            prob = [1/len(x) for x in unique_pos]\n",
        "            prob_norm = [x/sum(prob) for x in prob]\n",
        "            to_drop = np.random.choice(len(unique_pos), num_to_drop, p=prob_norm, replace=False)\n",
        "\n",
        "        drop_idx = []\n",
        "        for char_group in to_drop:\n",
        "            drop_idx += unique_pos[char_group]\n",
        "\n",
        "        target = np.clip(np.sum(word[drop_idx], axis=0), 0, 1)\n",
        "        assert(target[self.vocab_size] == 0)\n",
        "        target = target[:-1]\n",
        "\n",
        "        input_vec = np.copy(word)\n",
        "        input_vec[drop_idx] = self.blank_vec\n",
        "\n",
        "        if self.use_embedding:\n",
        "            input_vec = np.argmax(input_vec, axis=1)\n",
        "\n",
        "        not_present = np.array(sorted(list(all_chars - chars)))\n",
        "        num_misses = np.random.randint(0, 10)\n",
        "        miss_chars = np.random.choice(not_present, num_misses)\n",
        "        miss_chars = list(set([self.char_to_id[x] for x in miss_chars]))\n",
        "\n",
        "        miss_vec = np.zeros((self.vocab_size))\n",
        "        miss_vec[miss_chars] = 1\n",
        "\n",
        "        return input_vec, target, miss_vec\n",
        "\n",
        "\n",
        "class WordDataLoader(DataLoader):\n",
        "    def __init__(self, mode, config):\n",
        "        self.dataset = WordDataset(mode, config)\n",
        "\n",
        "        collate_fn = lambda batch: WordDataLoader.collate_fn(batch, config['vocab_size'], config['use_embedding'])\n",
        "        super(WordDataLoader, self).__init__(self.dataset, batch_size=config['batch_size'], shuffle=True, num_workers=config['num_workers'], collate_fn=collate_fn)\n",
        "\n",
        "    def update_dataset(self, epoch):\n",
        "        self.dataset.update_epoch(epoch)\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch, vocab_size, use_embedding):\n",
        "        lens = np.array([len(x[0]) for x in batch])\n",
        "        inputs = batchify_words([x[0] for x in batch], vocab_size, use_embedding)\n",
        "        labels = np.array([x[1] for x in batch])\n",
        "        miss_chars = np.array([x[2] for x in batch])\n",
        "        return inputs, labels, miss_chars, lens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "gKO1yIO48kLZ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "The main driver file responsible for training, testing and predicting\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import yaml\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class dl_model():\n",
        "\n",
        "\tdef __init__(self, mode):\n",
        "\n",
        "\t\twith open(\"/home/leon/Documents/new_trex/config.yaml\", 'r') as stream:\n",
        "\t\t\ttry:\n",
        "\t\t\t\tself.config = yaml.safe_load(stream)\n",
        "\t\t\texcept yaml.YAMLError as exc:\n",
        "\t\t\t\tprint(exc)\n",
        "\t\tself.mode = mode\n",
        "\n",
        "\t\tfeature_dim = self.config['vocab_size']\n",
        "\t\tself.arch_name = '_'.join(\n",
        "\t\t\t[self.config['rnn'], str(self.config['num_layers']), str(self.config['hidden_dim']), str(feature_dim)])\n",
        "\n",
        "\t\tprint(\"Architecture:\", self.arch_name)\n",
        "\n",
        "\t\tself.config['models'] = os.path.join(self.config['models'], self.arch_name)\n",
        "\t\tself.config['plots'] = os.path.join(self.config['plots'], self.arch_name)\n",
        "\n",
        "\n",
        "\t\tif not os.path.exists(self.config['models']):\n",
        "\t\t\tos.mkdir(self.config['models'])\n",
        "\t\tif not os.path.exists(self.config['plots']):\n",
        "\t\t\tos.mkdir(self.config['plots'])\n",
        "\t\tif not os.path.exists(self.config['pickle']):\n",
        "\t\t\tos.mkdir(self.config['pickle'])\n",
        "\n",
        "\t\tself.cuda = (self.config['cuda'] and torch.cuda.is_available())\n",
        "\n",
        "\t\tif mode == 'train' or mode == 'test':\n",
        "\n",
        "\t\t\tself.plots_dir = self.config['plots']\n",
        "\t\n",
        "\t\t\tself.total_epochs = self.config['epochs']\n",
        "\t\t\tself.test_every = self.config['test_every_epoch']\n",
        "\t\t\tself.test_per = self.config['test_per_epoch']\n",
        "\t\t\tself.print_per = self.config['print_per_epoch']\n",
        "\t\t\tself.save_every = self.config['save_every']\n",
        "\t\t\tself.plot_every = self.config['plot_every']\n",
        "\n",
        "\t\t\tself.train_loader = WordDataLoader('train', self.config)\n",
        "\t\t\tself.test_loader = WordDataLoader('test', self.config)\n",
        "\t\n",
        "\t\t\tself.model = RNN(self.config)\n",
        "\n",
        "\t\t\tself.start_epoch = 1\n",
        "\t\t\tself.edit_dist = []\n",
        "\t\t\tself.train_losses, self.test_losses = [], []\n",
        "\n",
        "\t\telse:\n",
        "\n",
        "\t\t\tself.model = RNN(self.config)\n",
        "\n",
        "\t\tif self.cuda:\n",
        "\t\t\tself.model.cuda()\n",
        "\n",
        "\t\tif self.mode == 'train' and self.config['resume']:\n",
        "\t\t\tself.start_epoch, self.train_losses, self.test_losses = self.model.load_model(mode, self.model.rnn_name, self.model.num_layers, self.model.hidden_dim)\n",
        "\t\t\tself.start_epoch += 1\n",
        "\n",
        "\t\telif self.mode == 'test' or mode == 'test_one':\n",
        "\t\t\tself.model.load_model(mode, self.config['rnn'], self.model.num_layers, self.model.hidden_dim, epoch=None)\n",
        "\n",
        "\t\tif self.config['use_embedding']:\n",
        "\t\t\tself.use_embedding = True\n",
        "\t\telse:\n",
        "\t\t\tself.use_embedding = False\n",
        "\n",
        "\tdef train(self):\n",
        "\n",
        "\t\tself.model.train()\n",
        "\n",
        "\n",
        "\t\tprint_range = list(np.linspace(0, len(self.train_loader), self.print_per + 2, dtype=np.uint32)[1:-1])\n",
        "\t\tif self.test_per == 0:\n",
        "\t\t\ttest_range = []\n",
        "\t\telse:\n",
        "\t\t\ttest_range = list(np.linspace(0, len(self.train_loader), self.test_per + 2, dtype=np.uint32)[1:-1])\n",
        "\n",
        "\t\tfor epoch in range(self.start_epoch, self.total_epochs + 1):\n",
        "\n",
        "\t\t\tepoch_loss = 0.0\n",
        "\n",
        "\t\t\tpbar = tqdm(total=len(self.train_loader), desc='Epoch [%i/%i]' % (epoch, self.total_epochs), ncols=100)\n",
        "\n",
        "\t\t\tfor i, (inputs, labels, miss_chars, input_lens) in enumerate(self.train_loader):\n",
        "\t\t\t\tif self.use_embedding:\n",
        "\t\t\t\t\tinputs = torch.from_numpy(inputs).long()\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tinputs = torch.from_numpy(inputs).float()\n",
        "\n",
        "\t\t\t\tlabels = torch.from_numpy(labels).float()\n",
        "\t\t\t\tmiss_chars = torch.from_numpy(miss_chars).float()\n",
        "\t\t\t\tinput_lens = torch.from_numpy(input_lens).long()\n",
        "\n",
        "\t\t\t\tif self.cuda:\n",
        "\t\t\t\t\tinputs = inputs.cuda()\n",
        "\t\t\t\t\tlabels = labels.cuda()\n",
        "\t\t\t\t\tmiss_chars = miss_chars.cuda()\n",
        "\t\t\t\t\tinput_lens = input_lens.cuda()\n",
        "\n",
        "\t\t\t\tself.model.optimizer.zero_grad()\n",
        "\n",
        "\t\t\t\toutputs = self.model(inputs, input_lens, miss_chars)\n",
        "\t\t\t\tloss, miss_penalty = self.model.calculate_loss(outputs, labels, input_lens, miss_chars, self.cuda)\n",
        "\t\t\t\tloss.backward()\n",
        "\n",
        "\t\t\t\tself.model.optimizer.step()\n",
        "\n",
        "\t\t\t\tepoch_loss += loss.item()\n",
        "\n",
        "\t\t\t\tif i in print_range and epoch == 1:\n",
        "\t\t\t\t\tpbar.set_description('Epoch [%i/%i], Loss=%.4f' % (epoch, self.total_epochs, epoch_loss / i))\n",
        "\t\t\t\telif i in print_range and epoch > 1:\n",
        "\t\t\t\t\tpbar.set_description('Epoch [%i/%i], Loss=%.4f, Avg.L=%.4f, MissL=%.4f' % (\n",
        "\t\t\t\t\t\tepoch, self.total_epochs, epoch_loss / i, np.mean(np.array([x[0] for x in self.train_losses])), miss_penalty))\n",
        "\t\t\t\tpbar.update()\n",
        "\n",
        "\t\t\t\tif i in test_range:\n",
        "\t\t\t\t\tself.test(epoch)\n",
        "\t\t\t\t\tself.model.train()\n",
        "\n",
        "\n",
        "\t\t\tif epoch % self.config['reset_after'] == 0:\n",
        "\t\t\t\tself.train_loader.update_dataset(epoch)\n",
        "\n",
        "\t\t\tself.train_losses.append((epoch_loss / len(self.train_loader), epoch))\n",
        "\n",
        "\t\t\tif epoch % self.save_every == 0:\n",
        "\t\t\t\tself.model.save_model(False, epoch, self.train_losses, self.test_losses,\n",
        "\t\t\t\t\t\t\t\t\t\tself.model.rnn_name, self.model.num_layers, self.model.hidden_dim)\n",
        "\n",
        "\t\t\tif epoch % 5 == 0 and epoch < self.test_every:\n",
        "\t\t\t\tself.test(epoch)\n",
        "\t\t\t\tself.model.train()\n",
        "\t\t\telif epoch % self.test_every == 0:\n",
        "\t\t\t\tself.test(epoch)\n",
        "\t\t\t\tself.model.train()\n",
        "\n",
        "\t\t\tif epoch % self.plot_every == 0:\n",
        "\t\t\t\tself.plot_loss_acc(epoch)\n",
        "\n",
        "\tdef test(self, epoch=None):\n",
        "\n",
        "\t\tself.model.eval()\n",
        "\n",
        "\t\ttest_loss = 0\n",
        "\n",
        "\n",
        "\t\twith torch.no_grad():\n",
        "\n",
        "\t\t\tfor inputs, labels, miss_chars, input_lens in tqdm(self.test_loader, desc='Testing', ncols=100):\n",
        "\n",
        "\t\t\t\tif self.use_embedding:\n",
        "\t\t\t\t\tinputs = torch.from_numpy(inputs).long()\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tinputs = torch.from_numpy(inputs).float()\n",
        "\n",
        "\t\t\t\tlabels = torch.from_numpy(labels).float()\n",
        "\t\t\t\tmiss_chars = torch.from_numpy(miss_chars).float()\n",
        "\t\t\t\tinput_lens= torch.from_numpy(input_lens).long()\n",
        "\n",
        "\t\t\t\tif self.cuda:\n",
        "\t\t\t\t\tinputs = inputs.cuda()\n",
        "\t\t\t\t\tlabels = labels.cuda()\n",
        "\t\t\t\t\tmiss_chars = miss_chars.cuda()\n",
        "\t\t\t\t\tinput_lens = input_lens.cuda()\n",
        "\n",
        "\t\t\t\tself.model.optimizer.zero_grad()\n",
        "\t\n",
        "\t\t\t\toutputs = self.model(inputs, input_lens, miss_chars)\n",
        "\t\t\t\tloss, miss_penalty = self.model.calculate_loss(outputs, labels, input_lens, miss_chars, self.cuda)\n",
        "\t\t\t\ttest_loss += loss.item()\n",
        "\n",
        "\t\n",
        "\t\ttest_loss /= len(self.test_loader)\n",
        "\n",
        "\t\tprint(\"Test Loss: %.7f, Miss Penalty: %.7f\" % (test_loss, miss_penalty))\n",
        "\n",
        "\n",
        "\t\tself.test_losses.append((test_loss, epoch))\n",
        "\n",
        "\t\tif test_loss == min([x[0] for x in self.test_losses]) and self.mode == 'train':\n",
        "\t\t\tprint(\"Best new model found!\")\n",
        "\t\t\tself.model.save_model(True, epoch, self.train_losses, self.test_losses,\n",
        "\t\t\t\t\t\t\t\t  self.model.rnn_name, self.model.num_layers, self.model.hidden_dim)\n",
        "\n",
        "\t\treturn test_loss\n",
        "\n",
        "\tdef predict(self, string, misses):\n",
        "\t\t\"\"\"\n",
        "\t\tcalled during inference\n",
        "\t\t:param string: word with predicted characters and blanks at remaining places\n",
        "\t\t:param misses: list of characters which were predicted but game feedback indicated that they are not present\n",
        "\t\t:param char_to_id: mapping from characters to id\n",
        "\t\t\"\"\"\n",
        "\t\n",
        "\t\tchar_to_id = {chr(97+x): x for x in range(26)}\n",
        "\t\t\n",
        "\t\tchar_to_id['.'] = len(char_to_id)\n",
        "\n",
        "\t\tid_to_char = {v:k for k,v in char_to_id.items()}\n",
        "\n",
        "\n",
        "\t\tif self.use_embedding:\n",
        "\t\t\tencoded = np.zeros((len(string)))\n",
        "\t\t\tfor i, c in enumerate(string):\n",
        "\t\t\t\tif c == '.':\n",
        "\t\t\t\t\tencoded[i] = len(id_to_char) - 1\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tencoded[i] = char_to_id[c]\n",
        "\n",
        "\t\t\tinputs = np.array(encoded)[None, :]\n",
        "\t\t\tinputs = torch.from_numpy(inputs).long()\n",
        "\n",
        "\t\telse:\n",
        "\n",
        "\t\t\tencoded = np.zeros((len(string), len(char_to_id)))\n",
        "\t\t\tfor i, c in enumerate(string):\n",
        "\t\t\t\tif c == '.':\n",
        "\t\t\t\t\tencoded[i][len(id_to_char) - 1] = 1\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tencoded[i][char_to_id[c]] = 1\n",
        "\n",
        "\t\t\tinputs = np.array(encoded)[None, :, :]\n",
        "\t\t\tinputs = torch.from_numpy(inputs).float()\n",
        "\n",
        "\t\t\n",
        "\t\tmiss_encoded = np.zeros((len(char_to_id) - 1))\n",
        "\t\tfor c in misses:\n",
        "\t\t\tmiss_encoded[char_to_id[c]] = 1\n",
        "\t\tmiss_encoded = np.array(miss_encoded)[None, :]\n",
        "\t\tmiss_encoded = torch.from_numpy(miss_encoded).float()\n",
        "\n",
        "\t\tinput_lens = np.array([len(string)])\n",
        "\t\tinput_lens= torch.from_numpy(input_lens).long()\n",
        "\n",
        "\t\tif self.cuda:\n",
        "\t\t\tinputs = inputs.cuda()\n",
        "\t\t\tmiss_encoded = miss_encoded.cuda()\n",
        "\t\t\tinput_lens = input_lens.cuda()\n",
        "\n",
        "\t\toutput = self.model(inputs, input_lens, miss_encoded)\n",
        "\t\tprobs = nn.functional.softmax(output, dim=1)\n",
        "\t\toutput = output.detach().cpu().numpy()[0]\n",
        "\t\tprobs = probs.detach().cpu().numpy()[0]\n",
        "\t\n",
        "\t\tsorted_predictions = np.argsort(output)[::-1]\n",
        "\n",
        "\t\treturn [id_to_char[x] for x in sorted_predictions], probs\n",
        "\n",
        "\tdef plot_loss_acc(self, epoch):\n",
        "\t\t\"\"\"\n",
        "\t\ttake train/test loss and test accuracy input and plot it over time\n",
        "\t\t:param epoch: to track performance across epochs\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tplt.clf()\n",
        "\t\tfig, ax1 = plt.subplots()\n",
        "\n",
        "\t\tax1.set_xlabel('Epoch')\n",
        "\t\tax1.set_ylabel('Loss')\n",
        "\t\tax1.plot([x[1] for x in self.train_losses], [x[0] for x in self.train_losses], color='r', label='Train Loss')\n",
        "\t\tax1.plot([x[1] for x in self.test_losses], [x[0] for x in self.test_losses], color='b', label='Test Loss')\n",
        "\t\tax1.tick_params(axis='y')\n",
        "\t\tax1.legend(loc='upper left')\n",
        "\n",
        "\t\tfig.tight_layout() \n",
        "\t\tplt.grid(True)\n",
        "\t\tplt.legend()\n",
        "\t\tplt.title(self.arch_name)\n",
        "\n",
        "\t\tfilename = os.path.join(self.plots_dir, 'plot_' + self.arch_name + '_' + str(epoch) + '.png')\n",
        "\t\tplt.savefig(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import collections\n",
        "from tqdm import tqdm\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "import collections\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "freq_table = {\n",
        "    1: ['a', 'i'],2: ['a', 'o', 'e', 'i', 'u', 'm', 'b', 'h'],3: ['a', 'e', 'o', 'i', 'u', 'y', 'h', 'b', 'c', 'k'],4: ['a', 'e', 'o', 'i', 'u', 'y', 's', 'b', 'f'],\n",
        "    5: ['s', 'e', 'a', 'o', 'i', 'u', 'y', 'h'],6: ['e', 'a', 'i', 'o', 'u', 's', 'y'],7: ['e', 'i', 'a', 'o', 'u', 's'],8: ['e', 'i', 'a', 'o', 'u'],\n",
        "    9: ['e', 'i', 'a', 'o', 'u'],10: ['e', 'i', 'o', 'a', 'u'],11: ['e', 'i', 'o', 'a', 'd'],12: ['e', 'i', 'o', 'a', 'f'],13: ['i', 'e', 'o', 'a'],14: ['i', 'e', 'o'],15: ['i', 'e', 'a'],\n",
        "    16: ['i', 'e', 'h'],17: ['i', 'e', 'r'],18: ['i', 'e', 'a'],19: ['i', 'e', 'a'],20: ['i', 'e']\n",
        "}\n",
        "#selected based on frequency of letters that occur for each word length in the train dataset\n",
        "\n",
        "def build_n_gram(word_list, max_n):\n",
        "    # create n-gram from word list\n",
        "    n_grams = {}\n",
        "    for n in range(1, max_n + 1):\n",
        "        n_grams[n] = collections.defaultdict(int)\n",
        "        for word in word_list:\n",
        "            for i in range(len(word) - n + 1):\n",
        "                n_grams[n][word[i:i + n]] += 1\n",
        "    return n_grams\n",
        "\n",
        "\n",
        "def build_n_gram_from_file(file_path, max_n=5):\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "        word_list = f.read().splitlines()\n",
        "    return build_n_gram(word_list, max_n)\n",
        "\n",
        "\n",
        "def get_n_gram_prob(n_grams, word, guessed_letters):\n",
        "\n",
        "    not_guessed_letters = [i for i in range(26) if chr(97 + i) not in guessed_letters]\n",
        "\n",
        "    next_letter_count = np.zeros(26, dtype=float)\n",
        "    alphas = [0.05, 0.1, 0.2, 0.3, 0.5]\n",
        "\n",
        "    gram_1_count = np.array([n_grams[1][chr(97 + j)] if j in not_guessed_letters else 0 for j in range(26)])\n",
        "    next_letter_count += alphas[0] * (gram_1_count / sum(gram_1_count))\n",
        "\n",
        "    for i in range(len(word) - 1):\n",
        "\n",
        "        gram_2_count = np.zeros(26)\n",
        "        if word[i] == '.' and word[i+1] != '.':\n",
        "            gram_2_count = np.array([n_grams[2][chr(97 + j) + word[i+1]] if j in not_guessed_letters else 0 for j in range(26)])\n",
        "        elif word[i] != '.' and word[i+1] == '.':\n",
        "            gram_2_count = np.array([n_grams[2][word[i] + chr(97 + j)] if j in not_guessed_letters else 0 for j in range(26)])\n",
        "        if sum(gram_2_count) != 0:\n",
        "            next_letter_count += alphas[1] * (gram_2_count / sum(gram_2_count))\n",
        "\n",
        "    for i in range(len(word) - 2):\n",
        "        gram_3_count = np.zeros(26)\n",
        " \n",
        "        if word[i] == '.' and word[i+1] != '.' and word[i+2] != '.':\n",
        "            gram_3_count = np.array([n_grams[3][chr(97 + j) + word[i+1:i+3]] if j in not_guessed_letters else 0 for j in range(26)])\n",
        "        elif word[i] != '.' and word[i+1] == '.' and word[i+2] != '.':\n",
        "            gram_3_count = np.array([n_grams[3][word[i] + chr(97 + j) + word[i+2]] if j in not_guessed_letters else 0 for j in range(26)])\n",
        "        elif word[i] != '.' and word[i+1] != '.' and word[i+2] == '.':\n",
        "            gram_3_count = np.array([n_grams[3][word[i:i+2] + chr(97 + j)] if j in not_guessed_letters else 0 for j in range(26)])\n",
        "        if sum(gram_3_count) != 0:\n",
        "            next_letter_count += alphas[2] * (gram_3_count / sum(gram_3_count))\n",
        "\n",
        "    for i in range(len(word) - 3):\n",
        "        gram_4_count = np.zeros(26)\n",
        "\n",
        "        if word[i] == '.' and word[i+1] != '.' and word[i+2] != '.' and word[i+3] != '.':\n",
        "            gram_4_count = np.array([n_grams[4][chr(97 + j) + word[i+1:i+4]] if j in not_guessed_letters else 0 for j in range(26)])\n",
        "        elif word[i] != '.' and word[i+1] == '.' and word[i+2] != '.' and word[i+3] != '.':\n",
        "            gram_4_count = np.array([n_grams[4][word[i] + chr(97 + j) + word[i+2:i+4]] if j in not_guessed_letters else 0 for j in range(26)])\n",
        "        elif word[i] != '.' and word[i+1] != '.' and word[i+2] == '.' and word[i+3] != '.':\n",
        "            gram_4_count = np.array([n_grams[4][word[i:i+2] + chr(97 + j) + word[i+3]] if j in not_guessed_letters else 0 for j in range(26)])\n",
        "        elif word[i] != '.' and word[i+1] != '.' and word[i+2] != '.' and word[i+3] == '.':\n",
        "            gram_4_count = np.array([n_grams[4][word[i:i+3] + chr(97 + j)] if j in not_guessed_letters else 0 for j in range(26)])\n",
        "        \n",
        "        if sum(gram_4_count) != 0:\n",
        "            next_letter_count += alphas[3] * (gram_4_count / sum(gram_4_count))\n",
        "\n",
        "        gram_4_2_count = np.zeros(26)\n",
        "\n",
        "        if word[i] == '.' and word[i+1] == '.' and word[i+2] != '.' and word[i+3] != '.':\n",
        "            for j in range(26):\n",
        "                for k in range(26):\n",
        "                    if j in not_guessed_letters and k in not_guessed_letters:\n",
        "                        gram_4_2_count[j] += n_grams[4][chr(97 + j) + chr(97 + k) + word[i+2:i+4]]\n",
        "                        gram_4_2_count[k] += n_grams[4][chr(97 + j) + chr(97 + k) + word[i+2:i+4]]\n",
        "        elif word[i] != '.' and word[i+1] == '.' and word[i+2] == '.' and word[i+3] != '.':\n",
        "            for j in range(26):\n",
        "                for k in range(26):\n",
        "                    if j in not_guessed_letters and k in not_guessed_letters:\n",
        "                        gram_4_2_count[j] += n_grams[4][word[i] + chr(97 + j) + chr(97 + k) + word[i+3]]\n",
        "                        gram_4_2_count[k] += n_grams[4][word[i] + chr(97 + j) + chr(97 + k) + word[i+3]]\n",
        "        elif word[i] != '.' and word[i+1] != '.' and word[i+2] == '.' and word[i+3] == '.':\n",
        "            for j in range(26):\n",
        "                for k in range(26):\n",
        "                    if j in not_guessed_letters and k in not_guessed_letters:\n",
        "                        gram_4_2_count[j] += n_grams[4][word[i:i+2] + chr(97 + j) + chr(97 + k)]\n",
        "                        gram_4_2_count[k] += n_grams[4][word[i:i+2] + chr(97 + j) + chr(97 + k)]\n",
        "        elif word[i] == '.' and word[i+1] != '.' and word[i+2] == '.' and word[i+3] != '.':\n",
        "            for j in range(26):\n",
        "                for k in range(26):\n",
        "                    if j in not_guessed_letters and k in not_guessed_letters:\n",
        "                        gram_4_2_count[j] += n_grams[4][chr(97 + j) + word[i+1] + chr(97 + k) + word[i+3]]\n",
        "                        gram_4_2_count[k] += n_grams[4][chr(97 + j) + word[i+1] + chr(97 + k) + word[i+3]]\n",
        "        elif word[i] != '.' and word[i+1] == '.' and word[i+2] != '.' and word[i+3] == '.':\n",
        "            for j in range(26):\n",
        "                for k in range(26):\n",
        "                    if j in not_guessed_letters and k in not_guessed_letters:\n",
        "                        gram_4_2_count[j] += n_grams[4][word[i] + chr(97 + j) + word[i+2] + chr(97 + k)]\n",
        "                        gram_4_2_count[k] += n_grams[4][word[i] + chr(97 + j) + word[i+2] + chr(97 + k)]\n",
        "        elif word[i] == '.' and word[i+1] != '.' and word[i+2] != '.' and word[i+3] == '.':\n",
        "            for j in range(26):\n",
        "                for k in range(26):\n",
        "                    if j in not_guessed_letters and k in not_guessed_letters:\n",
        "                        gram_4_2_count[j] += n_grams[4][chr(97 + j) + word[i+1:i+3] + chr(97 + k)]\n",
        "                        gram_4_2_count[k] += n_grams[4][chr(97 + j) + word[i+1:i+3] + chr(97 + k)]\n",
        "        \n",
        "        if sum(gram_4_2_count) != 0:\n",
        "            next_letter_count += (alphas[3] / 2) * (gram_4_2_count / sum(gram_4_2_count))\n",
        "\n",
        "    for i in range(len(word) - 4):\n",
        "        dot_count = sum([1 for c in word[i:i+5] if c == '.'])\n",
        "        \n",
        "        if dot_count == 1:\n",
        "            gram_5_count = np.zeros(26)\n",
        "\n",
        "            if word[i] == '.' and word[i+1] != '.' and word[i+2] != '.' and word[i+3] != '.' and word[i+4] != '.':\n",
        "                gram_5_count = np.array([n_grams[5][chr(97 + j) + word[i+1:i+5]] if j in not_guessed_letters else 0 for j in range(26)])\n",
        "            elif word[i] != '.' and word[i+1] == '.' and word[i+2] != '.' and word[i+3] != '.' and word[i+4] != '.':\n",
        "                gram_5_count = np.array([n_grams[5][word[i] + chr(97 + j) + word[i+2:i+5]] if j in not_guessed_letters else 0 for j in range(26)])\n",
        "            elif word[i] != '.' and word[i+1] != '.' and word[i+2] == '.' and word[i+3] != '.' and word[i+4] != '.':\n",
        "                gram_5_count = np.array([n_grams[5][word[i:i+2] + chr(97 + j) + word[i+3:i+5]] if j in not_guessed_letters else 0 for j in range(26)])\n",
        "            elif word[i] != '.' and word[i+1] != '.' and word[i+2] != '.' and word[i+3] == '.' and word[i+4] != '.':\n",
        "                gram_5_count = np.array([n_grams[5][word[i:i+3] + chr(97 + j) + word[i+4]] if j in not_guessed_letters else 0 for j in range(26)])\n",
        "            elif word[i] != '.' and word[i+1] != '.' and word[i+2] != '.' and word[i+3] != '.' and word[i+4] == '.':\n",
        "                gram_5_count = np.array([n_grams[5][word[i:i+4] + chr(97 + j)] if j in not_guessed_letters else 0 for j in range(26)])\n",
        "            if sum(gram_5_count) != 0:\n",
        "                next_letter_count += alphas[4] * (gram_5_count / sum(gram_5_count))\n",
        "\n",
        "        elif dot_count == 2:\n",
        "            gram_5_2_count = np.zeros(26)\n",
        "            if word[i] == '.' and word[i+1] == '.' and word[i+2] != '.' and word[i+3] != '.' and word[i+4] != '.':\n",
        "                for j in range(26):\n",
        "                    for k in range(26):\n",
        "                        if j in not_guessed_letters and k in not_guessed_letters:\n",
        "                            gram_5_2_count[j] += n_grams[5][chr(97 + j) + chr(97 + k) + word[i+2:i+5]]\n",
        "                            gram_5_2_count[k] += n_grams[5][chr(97 + j) + chr(97 + k) + word[i+2:i+5]]\n",
        "            elif word[i] != '.' and word[i+1] == '.' and word[i+2] == '.' and word[i+3] != '.' and word[i+4] != '.':\n",
        "                for j in range(26):\n",
        "                    for k in range(26):\n",
        "                        if j in not_guessed_letters and k in not_guessed_letters:\n",
        "                            gram_5_2_count[j] += n_grams[5][word[i] + chr(97 + j) + chr(97 + k) + word[i+3:i+5]]\n",
        "                            gram_5_2_count[k] += n_grams[5][word[i] + chr(97 + j) + chr(97 + k) + word[i+3:i+5]]\n",
        "            elif word[i] != '.' and word[i+1] != '.' and word[i+2] == '.' and word[i+3] == '.' and word[i+4] != '.':\n",
        "                for j in range(26):\n",
        "                    for k in range(26):\n",
        "                        if j in not_guessed_letters and k in not_guessed_letters:\n",
        "                            gram_5_2_count[j] += n_grams[5][word[i:i+2] + chr(97 + j) + chr(97 + k) + word[i+4]]\n",
        "                            gram_5_2_count[k] += n_grams[5][word[i:i+2] + chr(97 + j) + chr(97 + k) + word[i+4]]\n",
        "            elif word[i] != '.' and word[i+1] != '.' and word[i+2] != '.' and word[i+3] == '.' and word[i+4] == '.':\n",
        "                for j in range(26):\n",
        "                    for k in range(26):\n",
        "                        if j in not_guessed_letters and k in not_guessed_letters:\n",
        "                            gram_5_2_count[j] += n_grams[5][word[i:i+3] + chr(97 + j) + chr(97 + k)]\n",
        "                            gram_5_2_count[k] += n_grams[5][word[i:i+3] + chr(97 + j) + chr(97 + k)]\n",
        "            elif word[i] == '.' and word[i+1] != '.' and word[i+2] == '.' and word[i+3] != '.' and word[i+4] != '.':\n",
        "                for j in range(26):\n",
        "                    for k in range(26):\n",
        "                        if j in not_guessed_letters and k in not_guessed_letters:\n",
        "                            gram_5_2_count[j] += n_grams[5][chr(97 + j) + word[i+1] + chr(97 + k) + word[i+3:i+5]]\n",
        "                            gram_5_2_count[k] += n_grams[5][chr(97 + j) + word[i+1] + chr(97 + k) + word[i+3:i+5]]\n",
        "            elif word[i] == '.' and word[i+1] != '.' and word[i+2] != '.' and word[i+3] == '.' and word[i+4] != '.':\n",
        "                for j in range(26):\n",
        "                    for k in range(26):\n",
        "                        if j in not_guessed_letters and k in not_guessed_letters:\n",
        "                            gram_5_2_count[j] += n_grams[5][chr(97 + j) + word[i+1:i+3] + chr(97 + k) + word[i+4]]\n",
        "                            gram_5_2_count[k] += n_grams[5][chr(97 + j) + word[i+1:i+3] + chr(97 + k) + word[i+4]]\n",
        "            elif word[i] == '.' and word[i+1] != '.' and word[i+2] != '.' and word[i+3] != '.' and word[i+4] == '.':\n",
        "                for j in range(26):\n",
        "                    for k in range(26):\n",
        "                        if j in not_guessed_letters and k in not_guessed_letters:\n",
        "                            gram_5_2_count[j] += n_grams[5][chr(97 + j) + word[i+1:i+4] + chr(97 + k)]\n",
        "                            gram_5_2_count[k] += n_grams[5][chr(97 + j) + word[i+1:i+4] + chr(97 + k)]\n",
        "            elif word[i] != '.' and word[i+1] == '.' and word[i+2] != '.' and word[i+3] == '.' and word[i+4] != '.':\n",
        "                for j in range(26):\n",
        "                    for k in range(26):\n",
        "                        if j in not_guessed_letters and k in not_guessed_letters:\n",
        "                            gram_5_2_count[j] += n_grams[5][word[i] + chr(97 + j) + word[i+2] + chr(97 + k) + word[i+4]]\n",
        "                            gram_5_2_count[k] += n_grams[5][word[i] + chr(97 + j) + word[i+2] + chr(97 + k) + word[i+4]]\n",
        "            elif word[i] != '.' and word[i+1] == '.' and word[i+2] != '.' and word[i+3] != '.' and word[i+4] == '.':\n",
        "                for j in range(26):\n",
        "                    for k in range(26):\n",
        "                        if j in not_guessed_letters and k in not_guessed_letters:\n",
        "                            gram_5_2_count[j] += n_grams[5][word[i] + chr(97 + j) + word[i+2:i+4] + chr(97 + k)]\n",
        "                            gram_5_2_count[k] += n_grams[5][word[i] + chr(97 + j) + word[i+2:i+4] + chr(97 + k)]\n",
        "            elif word[i] != '.' and word[i+1] != '.' and word[i+2] == '.' and word[i+3] != '.' and word[i+4] == '.':\n",
        "                for j in range(26):\n",
        "                    for k in range(26):\n",
        "                        if j in not_guessed_letters and k in not_guessed_letters:\n",
        "                            gram_5_2_count[j] += n_grams[5][word[i:i+2] + chr(97 + j) + word[i+3] + chr(97 + k)]\n",
        "                            gram_5_2_count[k] += n_grams[5][word[i:i+2] + chr(97 + j) + word[i+3] + chr(97 + k)]\n",
        "            \n",
        "            if sum(gram_5_2_count) != 0:\n",
        "                next_letter_count += (alphas[4] / 2) * (gram_5_2_count / sum(gram_5_2_count))\n",
        "\n",
        "        next_letter_count /= sum(next_letter_count)\n",
        "\n",
        "    return next_letter_count\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "import random\n",
        "import string\n",
        "import secrets\n",
        "import time\n",
        "import re\n",
        "import collections\n",
        "\n",
        "try:\n",
        "    from urllib.parse import parse_qs, urlencode, urlparse\n",
        "except ImportError:\n",
        "    from urlparse import parse_qs, urlparse\n",
        "    from urllib import urlencode\n",
        "\n",
        "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
        "\n",
        "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HangmanAPI(object):\n",
        "    def __init__(self, access_token=None, session=None, timeout=None):\n",
        "        self.hangman_url = self.determine_hangman_url()\n",
        "        self.access_token = access_token\n",
        "        self.session = session or requests.Session()\n",
        "        self.timeout = timeout\n",
        "        self.guessed_letters = []\n",
        "        self.misses = []\n",
        "        \n",
        "        full_dictionary_location = \"words_250000_train.txt\"\n",
        "        self.full_dictionary = self.build_dictionary(full_dictionary_location)        \n",
        "        self.full_dictionary_common_letter_sorted = collections.Counter(\"\".join(self.full_dictionary)).most_common()\n",
        "        self.model = dl_model('test_one')\n",
        "        self.current_dictionary = []\n",
        "        self.n_grams = build_n_gram_from_file(full_dictionary_location)\n",
        "\n",
        "    @staticmethod\n",
        "    def determine_hangman_url():\n",
        "        links = ['https://trexsim.com', 'https://sg.trexsim.com']\n",
        "\n",
        "        data = {link: 0 for link in links}\n",
        "\n",
        "        for link in links:\n",
        "\n",
        "            requests.get(link)\n",
        "\n",
        "            for i in range(10):\n",
        "                s = time.time()\n",
        "                requests.get(link)\n",
        "                data[link] = time.time() - s\n",
        "\n",
        "        link = sorted(data.items(), key=lambda x: x[1])[0][0]\n",
        "        link += '/trexsim/hangman'\n",
        "        return link\n",
        "\n",
        "    def guess(self, word): # word input example: \"_ p p _ e \"\n",
        "        ###############################################\n",
        "        # Replace with your own \"guess\" function here #\n",
        "        ###############################################\n",
        "\n",
        "        # clean the word so that we strip away the space characters\n",
        "        # replace \"_\" with \".\" as \".\" indicates any character in regular expressions\n",
        "        self.misses = []\n",
        "        for char in self.guessed_letters:\n",
        "            # Check if the character is not in the word\n",
        "            if char not in word and char != '_':\n",
        "                # Append to missed_letters list\n",
        "                self.misses.append(char)\n",
        "\n",
        "        clean_word = word[::2].replace(\"_\",\".\")\n",
        "        len_word = len(clean_word)\n",
        "        len_right_letters = len(clean_word) - clean_word.count('.')\n",
        "    \n",
        "        if len_right_letters == 0 and len_word in freq_table:\n",
        "            order = freq_table[len_word]\n",
        "            for letter in order:\n",
        "                if letter not in self.guessed_letters:\n",
        "                    # print(\"first guess: \", letter)\n",
        "                    return letter \n",
        "        \n",
        "        ngram_probab = get_n_gram_prob(self.n_grams, clean_word, self.guessed_letters)\n",
        "\n",
        "        best_chars, nn_probab = self.model.predict(clean_word, self.misses)\n",
        "\n",
        "        nn_probab = [p if chr(i+97) not in self.misses and chr(i+97) not in clean_word else 0.0 for i,p in enumerate(nn_probab)]\n",
        "        nn_probab = [p/sum(nn_probab) for p in nn_probab]\n",
        "\n",
        "        final_probab = nn_probab + ngram_probab\n",
        "        guess_letter = chr(final_probab.argmax() + 97)\n",
        "\n",
        "        return guess_letter\n",
        "\n",
        "    ##########################################################\n",
        "    # You'll likely not need to modify any of the code below #\n",
        "    ##########################################################\n",
        "    \n",
        "    def build_dictionary(self, dictionary_file_location):\n",
        "        text_file = open(dictionary_file_location,\"r\")\n",
        "        full_dictionary = text_file.read().splitlines()\n",
        "        text_file.close()\n",
        "        return full_dictionary\n",
        "                \n",
        "    def start_game(self, practice=True, verbose=True):\n",
        "        # reset guessed letters to empty set and current plausible dictionary to the full dictionary\n",
        "        self.guessed_letters = []\n",
        "        self.misses = []\n",
        "        self.current_dictionary = self.full_dictionary\n",
        "                         \n",
        "        response = self.request(\"/new_game\", {\"practice\":practice})\n",
        "        if response.get('status')==\"approved\":\n",
        "            game_id = response.get('game_id')\n",
        "            word = response.get('word')\n",
        "            tries_remains = response.get('tries_remains')\n",
        "            if verbose:\n",
        "                print(\"Successfully start a new game! Game ID: {0}. # of tries remaining: {1}. Word: {2}.\".format(game_id, tries_remains, word))\n",
        "            while tries_remains>0:\n",
        "                # get guessed letter from user code\n",
        "                guess_letter = self.guess(word)\n",
        "                    \n",
        "                # append guessed letter to guessed letters field in hangman object\n",
        "                self.guessed_letters.append(guess_letter)\n",
        "                if verbose:\n",
        "                    print(\"Guessing letter: {0}\".format(guess_letter))\n",
        "                    \n",
        "                try:    \n",
        "                    res = self.request(\"/guess_letter\", {\"request\":\"guess_letter\", \"game_id\":game_id, \"letter\":guess_letter})\n",
        "                except HangmanAPIError:\n",
        "                    print('HangmanAPIError exception caught on request.')\n",
        "                    continue\n",
        "                except Exception as e:\n",
        "                    print('Other exception caught on request.')\n",
        "                    raise e\n",
        "               \n",
        "                if verbose:\n",
        "                    print(\"Sever response: {0}\".format(res))\n",
        "                status = res.get('status')\n",
        "                tries_remains = res.get('tries_remains')\n",
        "                if status==\"success\":\n",
        "                    if verbose:\n",
        "                        print(\"Successfully finished game: {0}\".format(game_id))\n",
        "                    return True\n",
        "                elif status==\"failed\":\n",
        "                    reason = res.get('reason', '# of tries exceeded!')\n",
        "                    if verbose:\n",
        "                        print(\"Failed game: {0}. Because of: {1}\".format(game_id, reason))\n",
        "                    return False\n",
        "                elif status==\"ongoing\":\n",
        "                    word = res.get('word')\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(\"Failed to start a new game\")\n",
        "        return status==\"success\"\n",
        "        \n",
        "    def my_status(self):\n",
        "        return self.request(\"/my_status\", {})\n",
        "    \n",
        "    def request(\n",
        "            self, path, args=None, post_args=None, method=None):\n",
        "        if args is None:\n",
        "            args = dict()\n",
        "        if post_args is not None:\n",
        "            method = \"POST\"\n",
        "\n",
        "        # Add `access_token` to post_args or args if it has not already been\n",
        "        # included.\n",
        "        if self.access_token:\n",
        "            # If post_args exists, we assume that args either does not exists\n",
        "            # or it does not need `access_token`.\n",
        "            if post_args and \"access_token\" not in post_args:\n",
        "                post_args[\"access_token\"] = self.access_token\n",
        "            elif \"access_token\" not in args:\n",
        "                args[\"access_token\"] = self.access_token\n",
        "\n",
        "        time.sleep(0.2)\n",
        "\n",
        "        num_retry, time_sleep = 50, 2\n",
        "        for it in range(num_retry):\n",
        "            try:\n",
        "                response = self.session.request(\n",
        "                    method or \"GET\",\n",
        "                    self.hangman_url + path,\n",
        "                    timeout=self.timeout,\n",
        "                    params=args,\n",
        "                    data=post_args,\n",
        "                    verify=False\n",
        "                )\n",
        "                break\n",
        "            except requests.HTTPError as e:\n",
        "                response = json.loads(e.read())\n",
        "                raise HangmanAPIError(response)\n",
        "            except requests.exceptions.SSLError as e:\n",
        "                if it + 1 == num_retry:\n",
        "                    raise\n",
        "                time.sleep(time_sleep)\n",
        "\n",
        "        headers = response.headers\n",
        "        if 'json' in headers['content-type']:\n",
        "            result = response.json()\n",
        "        elif \"access_token\" in parse_qs(response.text):\n",
        "            query_str = parse_qs(response.text)\n",
        "            if \"access_token\" in query_str:\n",
        "                result = {\"access_token\": query_str[\"access_token\"][0]}\n",
        "                if \"expires\" in query_str:\n",
        "                    result[\"expires\"] = query_str[\"expires\"][0]\n",
        "            else:\n",
        "                raise HangmanAPIError(response.json())\n",
        "        else:\n",
        "            raise HangmanAPIError('Maintype was not text, or querystring')\n",
        "\n",
        "        if result and isinstance(result, dict) and result.get(\"error\"):\n",
        "            raise HangmanAPIError(result)\n",
        "        return result\n",
        "    \n",
        "class HangmanAPIError(Exception):\n",
        "    def __init__(self, result):\n",
        "        self.result = result\n",
        "        self.code = None\n",
        "        try:\n",
        "            self.type = result[\"error_code\"]\n",
        "        except (KeyError, TypeError):\n",
        "            self.type = \"\"\n",
        "\n",
        "        try:\n",
        "            self.message = result[\"error_description\"]\n",
        "        except (KeyError, TypeError):\n",
        "            try:\n",
        "                self.message = result[\"error\"][\"message\"]\n",
        "                self.code = result[\"error\"].get(\"code\")\n",
        "                if not self.type:\n",
        "                    self.type = result[\"error\"].get(\"type\", \"\")\n",
        "            except (KeyError, TypeError):\n",
        "                try:\n",
        "                    self.message = result[\"error_msg\"]\n",
        "                except (KeyError, TypeError):\n",
        "                    self.message = result\n",
        "\n",
        "        Exception.__init__(self, self.message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Architecture: GRU_4_512_26\n",
            "models/GRU_4_512_26/best_GRU_4_512.pth\n",
            "Loaded pretrained model from: models/GRU_4_512_26/best_GRU_4_512.pth\n"
          ]
        }
      ],
      "source": [
        "api = HangmanAPI(access_token=\"77554fdf3f995010f852fdde021f1d\", timeout=2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Playing  0  th game\n",
            "Playing  1  th game\n",
            "Playing  2  th game\n",
            "Playing  3  th game\n",
            "Playing  4  th game\n",
            "Playing  5  th game\n",
            "Playing  6  th game\n",
            "Playing  7  th game\n",
            "Playing  8  th game\n",
            "Playing  9  th game\n",
            "Playing  10  th game\n",
            "Playing  11  th game\n",
            "Playing  12  th game\n",
            "Playing  13  th game\n",
            "Playing  14  th game\n",
            "Playing  15  th game\n",
            "Playing  16  th game\n",
            "Playing  17  th game\n",
            "Playing  18  th game\n",
            "Playing  19  th game\n",
            "Playing  20  th game\n",
            "Playing  21  th game\n",
            "Playing  22  th game\n",
            "Playing  23  th game\n",
            "Playing  24  th game\n",
            "Playing  25  th game\n",
            "Playing  26  th game\n",
            "Playing  27  th game\n",
            "Playing  28  th game\n",
            "Playing  29  th game\n",
            "Playing  30  th game\n",
            "Playing  31  th game\n",
            "Playing  32  th game\n",
            "Playing  33  th game\n",
            "Playing  34  th game\n",
            "Playing  35  th game\n",
            "Playing  36  th game\n",
            "Playing  37  th game\n",
            "Playing  38  th game\n",
            "Playing  39  th game\n",
            "Playing  40  th game\n",
            "Playing  41  th game\n",
            "Playing  42  th game\n",
            "Playing  43  th game\n",
            "Playing  44  th game\n",
            "Playing  45  th game\n",
            "Playing  46  th game\n",
            "Playing  47  th game\n",
            "Playing  48  th game\n",
            "Playing  49  th game\n",
            "Playing  50  th game\n",
            "Playing  51  th game\n",
            "Playing  52  th game\n",
            "Playing  53  th game\n",
            "Playing  54  th game\n",
            "Playing  55  th game\n",
            "Playing  56  th game\n",
            "Playing  57  th game\n",
            "Playing  58  th game\n",
            "Playing  59  th game\n",
            "Playing  60  th game\n",
            "Playing  61  th game\n",
            "Playing  62  th game\n",
            "Playing  63  th game\n",
            "Playing  64  th game\n",
            "Playing  65  th game\n",
            "Playing  66  th game\n",
            "Playing  67  th game\n",
            "Playing  68  th game\n",
            "Playing  69  th game\n",
            "Playing  70  th game\n",
            "Playing  71  th game\n",
            "Playing  72  th game\n",
            "Playing  73  th game\n",
            "Playing  74  th game\n",
            "Playing  75  th game\n",
            "Playing  76  th game\n",
            "Playing  77  th game\n",
            "Playing  78  th game\n",
            "Playing  79  th game\n",
            "Playing  80  th game\n",
            "Playing  81  th game\n",
            "Playing  82  th game\n",
            "Playing  83  th game\n",
            "Playing  84  th game\n",
            "Playing  85  th game\n",
            "Playing  86  th game\n",
            "Playing  87  th game\n",
            "Playing  88  th game\n",
            "Playing  89  th game\n",
            "Playing  90  th game\n",
            "Playing  91  th game\n",
            "Playing  92  th game\n",
            "Playing  93  th game\n",
            "Playing  94  th game\n",
            "Playing  95  th game\n",
            "Playing  96  th game\n",
            "Playing  97  th game\n",
            "Playing  98  th game\n",
            "Playing  99  th game\n",
            "Playing  100  th game\n",
            "Playing  101  th game\n",
            "Playing  102  th game\n",
            "Playing  103  th game\n",
            "Playing  104  th game\n",
            "Playing  105  th game\n",
            "Playing  106  th game\n",
            "Playing  107  th game\n",
            "Playing  108  th game\n",
            "Playing  109  th game\n",
            "Playing  110  th game\n",
            "Playing  111  th game\n",
            "Playing  112  th game\n",
            "Playing  113  th game\n",
            "Playing  114  th game\n",
            "Playing  115  th game\n",
            "Playing  116  th game\n",
            "Playing  117  th game\n",
            "Playing  118  th game\n",
            "Playing  119  th game\n",
            "Playing  120  th game\n",
            "Playing  121  th game\n",
            "Playing  122  th game\n",
            "Playing  123  th game\n",
            "Playing  124  th game\n",
            "Playing  125  th game\n",
            "Playing  126  th game\n",
            "Playing  127  th game\n",
            "Playing  128  th game\n",
            "Playing  129  th game\n",
            "Playing  130  th game\n",
            "Playing  131  th game\n",
            "Playing  132  th game\n",
            "Playing  133  th game\n",
            "Playing  134  th game\n",
            "Playing  135  th game\n",
            "Playing  136  th game\n",
            "Playing  137  th game\n",
            "Playing  138  th game\n",
            "Playing  139  th game\n",
            "Playing  140  th game\n",
            "Playing  141  th game\n",
            "Playing  142  th game\n",
            "Playing  143  th game\n",
            "Playing  144  th game\n",
            "Playing  145  th game\n",
            "Playing  146  th game\n",
            "Playing  147  th game\n",
            "Playing  148  th game\n",
            "Playing  149  th game\n",
            "Playing  150  th game\n",
            "Playing  151  th game\n",
            "Playing  152  th game\n",
            "Playing  153  th game\n",
            "Playing  154  th game\n",
            "Playing  155  th game\n",
            "Playing  156  th game\n",
            "Playing  157  th game\n",
            "Playing  158  th game\n",
            "Playing  159  th game\n",
            "Playing  160  th game\n",
            "Playing  161  th game\n",
            "Playing  162  th game\n",
            "Playing  163  th game\n",
            "Playing  164  th game\n",
            "Playing  165  th game\n",
            "Playing  166  th game\n",
            "Playing  167  th game\n",
            "Playing  168  th game\n",
            "Playing  169  th game\n",
            "Playing  170  th game\n",
            "Playing  171  th game\n",
            "Playing  172  th game\n",
            "Playing  173  th game\n",
            "Playing  174  th game\n",
            "Playing  175  th game\n",
            "Playing  176  th game\n",
            "Playing  177  th game\n",
            "Playing  178  th game\n",
            "Playing  179  th game\n",
            "Playing  180  th game\n",
            "Playing  181  th game\n",
            "Playing  182  th game\n",
            "Playing  183  th game\n",
            "Playing  184  th game\n",
            "Playing  185  th game\n",
            "Playing  186  th game\n",
            "Playing  187  th game\n",
            "Playing  188  th game\n",
            "Playing  189  th game\n",
            "Playing  190  th game\n",
            "Playing  191  th game\n",
            "Playing  192  th game\n",
            "Playing  193  th game\n",
            "Playing  194  th game\n",
            "Playing  195  th game\n",
            "Playing  196  th game\n",
            "Playing  197  th game\n",
            "Playing  198  th game\n",
            "Playing  199  th game\n",
            "Playing  200  th game\n",
            "Playing  201  th game\n",
            "Playing  202  th game\n",
            "Playing  203  th game\n",
            "Playing  204  th game\n",
            "Playing  205  th game\n",
            "Playing  206  th game\n",
            "Playing  207  th game\n",
            "Playing  208  th game\n",
            "Playing  209  th game\n",
            "Playing  210  th game\n",
            "Playing  211  th game\n",
            "Playing  212  th game\n",
            "Playing  213  th game\n",
            "Playing  214  th game\n",
            "Playing  215  th game\n",
            "Playing  216  th game\n",
            "Playing  217  th game\n",
            "Playing  218  th game\n",
            "Playing  219  th game\n",
            "Playing  220  th game\n",
            "Playing  221  th game\n",
            "Playing  222  th game\n",
            "Playing  223  th game\n",
            "Playing  224  th game\n",
            "Playing  225  th game\n",
            "Playing  226  th game\n",
            "Playing  227  th game\n",
            "Playing  228  th game\n",
            "Playing  229  th game\n",
            "Playing  230  th game\n",
            "Playing  231  th game\n",
            "Playing  232  th game\n",
            "Playing  233  th game\n",
            "Playing  234  th game\n",
            "Playing  235  th game\n",
            "Playing  236  th game\n",
            "Playing  237  th game\n",
            "Playing  238  th game\n",
            "Playing  239  th game\n",
            "Playing  240  th game\n",
            "Playing  241  th game\n",
            "Playing  242  th game\n",
            "Playing  243  th game\n",
            "Playing  244  th game\n",
            "Playing  245  th game\n",
            "Playing  246  th game\n",
            "Playing  247  th game\n",
            "Playing  248  th game\n",
            "Playing  249  th game\n",
            "Playing  250  th game\n",
            "Playing  251  th game\n",
            "Playing  252  th game\n",
            "Playing  253  th game\n",
            "Playing  254  th game\n",
            "Playing  255  th game\n",
            "Playing  256  th game\n",
            "Playing  257  th game\n",
            "Playing  258  th game\n",
            "Playing  259  th game\n",
            "Playing  260  th game\n",
            "Playing  261  th game\n",
            "Playing  262  th game\n",
            "Playing  263  th game\n",
            "Playing  264  th game\n",
            "Playing  265  th game\n",
            "Playing  266  th game\n",
            "Playing  267  th game\n",
            "Playing  268  th game\n",
            "Playing  269  th game\n",
            "Playing  270  th game\n",
            "Playing  271  th game\n",
            "Playing  272  th game\n",
            "Playing  273  th game\n",
            "Playing  274  th game\n",
            "Playing  275  th game\n",
            "Playing  276  th game\n",
            "Playing  277  th game\n",
            "Playing  278  th game\n",
            "Playing  279  th game\n",
            "Playing  280  th game\n",
            "Playing  281  th game\n",
            "Playing  282  th game\n",
            "Playing  283  th game\n",
            "Playing  284  th game\n",
            "Playing  285  th game\n",
            "Playing  286  th game\n",
            "Playing  287  th game\n",
            "Playing  288  th game\n",
            "Playing  289  th game\n",
            "Playing  290  th game\n",
            "Playing  291  th game\n",
            "Playing  292  th game\n",
            "Playing  293  th game\n",
            "Playing  294  th game\n",
            "Playing  295  th game\n",
            "Playing  296  th game\n",
            "Playing  297  th game\n",
            "Playing  298  th game\n",
            "Playing  299  th game\n",
            "Playing  300  th game\n",
            "Playing  301  th game\n",
            "Playing  302  th game\n",
            "Playing  303  th game\n",
            "Playing  304  th game\n",
            "Playing  305  th game\n",
            "Playing  306  th game\n",
            "Playing  307  th game\n",
            "Playing  308  th game\n",
            "Playing  309  th game\n",
            "Playing  310  th game\n",
            "Playing  311  th game\n",
            "Playing  312  th game\n",
            "Playing  313  th game\n",
            "Playing  314  th game\n",
            "Playing  315  th game\n",
            "Playing  316  th game\n",
            "Playing  317  th game\n",
            "Playing  318  th game\n",
            "Playing  319  th game\n",
            "Playing  320  th game\n",
            "Playing  321  th game\n",
            "Playing  322  th game\n",
            "Playing  323  th game\n",
            "Playing  324  th game\n",
            "Playing  325  th game\n",
            "Playing  326  th game\n",
            "Playing  327  th game\n",
            "Playing  328  th game\n",
            "Playing  329  th game\n",
            "Playing  330  th game\n",
            "Playing  331  th game\n",
            "Playing  332  th game\n",
            "Playing  333  th game\n",
            "Playing  334  th game\n",
            "Playing  335  th game\n",
            "Playing  336  th game\n",
            "Playing  337  th game\n",
            "Playing  338  th game\n",
            "Playing  339  th game\n",
            "Playing  340  th game\n",
            "Playing  341  th game\n",
            "Playing  342  th game\n",
            "Playing  343  th game\n",
            "Playing  344  th game\n",
            "Playing  345  th game\n",
            "Playing  346  th game\n",
            "Playing  347  th game\n",
            "Playing  348  th game\n",
            "Playing  349  th game\n",
            "Playing  350  th game\n",
            "Playing  351  th game\n",
            "Playing  352  th game\n",
            "Playing  353  th game\n",
            "Playing  354  th game\n",
            "Playing  355  th game\n",
            "Playing  356  th game\n",
            "Playing  357  th game\n",
            "Playing  358  th game\n",
            "Playing  359  th game\n",
            "Playing  360  th game\n",
            "Playing  361  th game\n",
            "Playing  362  th game\n",
            "Playing  363  th game\n",
            "Playing  364  th game\n",
            "Playing  365  th game\n",
            "Playing  366  th game\n",
            "Playing  367  th game\n",
            "Playing  368  th game\n",
            "Playing  369  th game\n",
            "Playing  370  th game\n",
            "Playing  371  th game\n",
            "Playing  372  th game\n",
            "Playing  373  th game\n",
            "Playing  374  th game\n",
            "Playing  375  th game\n",
            "Playing  376  th game\n",
            "Playing  377  th game\n",
            "Playing  378  th game\n",
            "Playing  379  th game\n",
            "Playing  380  th game\n",
            "Playing  381  th game\n",
            "Playing  382  th game\n",
            "Playing  383  th game\n",
            "Playing  384  th game\n",
            "Playing  385  th game\n",
            "Playing  386  th game\n",
            "Playing  387  th game\n",
            "Playing  388  th game\n",
            "Playing  389  th game\n",
            "Playing  390  th game\n",
            "Playing  391  th game\n",
            "Playing  392  th game\n",
            "Playing  393  th game\n",
            "Playing  394  th game\n",
            "Playing  395  th game\n",
            "Playing  396  th game\n",
            "Playing  397  th game\n",
            "Playing  398  th game\n",
            "Playing  399  th game\n",
            "Playing  400  th game\n",
            "Playing  401  th game\n",
            "Playing  402  th game\n",
            "Playing  403  th game\n",
            "Playing  404  th game\n",
            "Playing  405  th game\n",
            "Playing  406  th game\n",
            "Playing  407  th game\n",
            "Playing  408  th game\n",
            "Playing  409  th game\n",
            "Playing  410  th game\n",
            "Playing  411  th game\n",
            "Playing  412  th game\n",
            "Playing  413  th game\n",
            "Playing  414  th game\n",
            "Playing  415  th game\n",
            "Playing  416  th game\n",
            "Playing  417  th game\n",
            "Playing  418  th game\n",
            "Playing  419  th game\n",
            "Playing  420  th game\n",
            "Playing  421  th game\n",
            "Playing  422  th game\n",
            "Playing  423  th game\n",
            "Playing  424  th game\n",
            "Playing  425  th game\n",
            "Playing  426  th game\n",
            "Playing  427  th game\n",
            "Playing  428  th game\n",
            "Playing  429  th game\n",
            "Playing  430  th game\n",
            "Playing  431  th game\n",
            "Playing  432  th game\n",
            "Playing  433  th game\n",
            "Playing  434  th game\n",
            "Playing  435  th game\n",
            "Playing  436  th game\n",
            "Playing  437  th game\n",
            "Playing  438  th game\n",
            "Playing  439  th game\n",
            "Playing  440  th game\n",
            "Playing  441  th game\n",
            "Playing  442  th game\n",
            "Playing  443  th game\n",
            "Playing  444  th game\n",
            "Playing  445  th game\n",
            "Playing  446  th game\n",
            "Playing  447  th game\n",
            "Playing  448  th game\n",
            "Playing  449  th game\n",
            "Playing  450  th game\n",
            "Playing  451  th game\n",
            "Playing  452  th game\n",
            "Playing  453  th game\n",
            "Playing  454  th game\n",
            "Playing  455  th game\n",
            "Playing  456  th game\n",
            "Playing  457  th game\n",
            "Playing  458  th game\n",
            "Playing  459  th game\n",
            "Playing  460  th game\n",
            "Playing  461  th game\n",
            "Playing  462  th game\n",
            "Playing  463  th game\n",
            "Playing  464  th game\n",
            "Playing  465  th game\n",
            "Playing  466  th game\n",
            "Playing  467  th game\n",
            "Playing  468  th game\n",
            "Playing  469  th game\n",
            "Playing  470  th game\n",
            "Playing  471  th game\n",
            "Playing  472  th game\n",
            "Playing  473  th game\n",
            "Playing  474  th game\n",
            "Playing  475  th game\n",
            "Playing  476  th game\n",
            "Playing  477  th game\n",
            "Playing  478  th game\n",
            "Playing  479  th game\n",
            "Playing  480  th game\n",
            "Playing  481  th game\n",
            "Playing  482  th game\n",
            "Playing  483  th game\n",
            "Playing  484  th game\n",
            "Playing  485  th game\n",
            "Playing  486  th game\n",
            "Playing  487  th game\n",
            "Playing  488  th game\n",
            "Playing  489  th game\n",
            "Playing  490  th game\n",
            "Playing  491  th game\n",
            "Playing  492  th game\n",
            "Playing  493  th game\n",
            "Playing  494  th game\n",
            "Playing  495  th game\n",
            "Playing  496  th game\n",
            "Playing  497  th game\n",
            "Playing  498  th game\n",
            "Playing  499  th game\n",
            "Playing  500  th game\n",
            "Playing  501  th game\n",
            "Playing  502  th game\n",
            "Playing  503  th game\n",
            "Playing  504  th game\n",
            "Playing  505  th game\n",
            "Playing  506  th game\n",
            "Playing  507  th game\n",
            "Playing  508  th game\n",
            "Playing  509  th game\n",
            "Playing  510  th game\n",
            "Playing  511  th game\n",
            "Playing  512  th game\n",
            "Playing  513  th game\n",
            "Playing  514  th game\n",
            "Playing  515  th game\n",
            "Playing  516  th game\n",
            "Playing  517  th game\n",
            "Playing  518  th game\n",
            "Playing  519  th game\n",
            "Playing  520  th game\n",
            "Playing  521  th game\n",
            "Playing  522  th game\n",
            "Playing  523  th game\n",
            "Playing  524  th game\n",
            "Playing  525  th game\n",
            "Playing  526  th game\n",
            "Playing  527  th game\n",
            "Playing  528  th game\n",
            "Playing  529  th game\n",
            "Playing  530  th game\n",
            "Playing  531  th game\n",
            "Playing  532  th game\n",
            "Playing  533  th game\n",
            "Playing  534  th game\n",
            "Playing  535  th game\n",
            "Playing  536  th game\n",
            "Playing  537  th game\n",
            "Playing  538  th game\n",
            "Playing  539  th game\n",
            "Playing  540  th game\n",
            "Playing  541  th game\n",
            "Playing  542  th game\n",
            "Playing  543  th game\n",
            "Playing  544  th game\n",
            "Playing  545  th game\n",
            "Playing  546  th game\n",
            "Playing  547  th game\n",
            "Playing  548  th game\n",
            "Playing  549  th game\n",
            "Playing  550  th game\n",
            "Playing  551  th game\n",
            "Playing  552  th game\n",
            "Playing  553  th game\n",
            "Playing  554  th game\n",
            "Playing  555  th game\n",
            "Playing  556  th game\n",
            "Playing  557  th game\n",
            "Playing  558  th game\n",
            "Playing  559  th game\n",
            "Playing  560  th game\n",
            "Playing  561  th game\n",
            "Playing  562  th game\n",
            "Playing  563  th game\n",
            "Playing  564  th game\n",
            "Playing  565  th game\n",
            "Playing  566  th game\n",
            "Playing  567  th game\n",
            "Playing  568  th game\n",
            "Playing  569  th game\n",
            "Playing  570  th game\n",
            "Playing  571  th game\n",
            "Playing  572  th game\n",
            "Playing  573  th game\n",
            "Playing  574  th game\n",
            "Playing  575  th game\n",
            "Playing  576  th game\n",
            "Playing  577  th game\n",
            "Playing  578  th game\n",
            "Playing  579  th game\n",
            "Playing  580  th game\n",
            "Playing  581  th game\n",
            "Playing  582  th game\n",
            "Playing  583  th game\n",
            "Playing  584  th game\n",
            "Playing  585  th game\n",
            "Playing  586  th game\n",
            "Playing  587  th game\n",
            "Playing  588  th game\n",
            "Playing  589  th game\n",
            "Playing  590  th game\n",
            "Playing  591  th game\n",
            "Playing  592  th game\n",
            "Playing  593  th game\n",
            "Playing  594  th game\n",
            "Playing  595  th game\n",
            "Playing  596  th game\n",
            "Playing  597  th game\n",
            "Playing  598  th game\n",
            "Playing  599  th game\n",
            "Playing  600  th game\n",
            "Playing  601  th game\n",
            "Playing  602  th game\n",
            "Playing  603  th game\n",
            "Playing  604  th game\n",
            "Playing  605  th game\n",
            "Playing  606  th game\n",
            "Playing  607  th game\n",
            "Playing  608  th game\n",
            "Playing  609  th game\n",
            "Playing  610  th game\n",
            "Playing  611  th game\n",
            "Playing  612  th game\n",
            "Playing  613  th game\n",
            "Playing  614  th game\n",
            "Playing  615  th game\n",
            "Playing  616  th game\n",
            "Playing  617  th game\n",
            "Playing  618  th game\n",
            "Playing  619  th game\n",
            "Playing  620  th game\n",
            "Playing  621  th game\n",
            "Playing  622  th game\n",
            "Playing  623  th game\n",
            "Playing  624  th game\n",
            "Playing  625  th game\n",
            "Playing  626  th game\n",
            "Playing  627  th game\n",
            "Playing  628  th game\n",
            "Playing  629  th game\n",
            "Playing  630  th game\n",
            "Playing  631  th game\n",
            "Playing  632  th game\n",
            "Playing  633  th game\n",
            "Playing  634  th game\n",
            "Playing  635  th game\n",
            "Playing  636  th game\n",
            "Playing  637  th game\n",
            "Playing  638  th game\n",
            "Playing  639  th game\n",
            "Playing  640  th game\n",
            "Playing  641  th game\n",
            "Playing  642  th game\n",
            "Playing  643  th game\n",
            "Playing  644  th game\n",
            "Playing  645  th game\n",
            "Playing  646  th game\n",
            "Playing  647  th game\n",
            "Playing  648  th game\n",
            "Playing  649  th game\n",
            "Playing  650  th game\n",
            "Playing  651  th game\n",
            "Playing  652  th game\n",
            "Playing  653  th game\n",
            "Playing  654  th game\n",
            "Playing  655  th game\n",
            "Playing  656  th game\n",
            "Playing  657  th game\n",
            "Playing  658  th game\n",
            "Playing  659  th game\n",
            "Playing  660  th game\n",
            "Playing  661  th game\n",
            "Playing  662  th game\n",
            "Playing  663  th game\n",
            "Playing  664  th game\n",
            "Playing  665  th game\n",
            "Playing  666  th game\n",
            "Playing  667  th game\n",
            "Playing  668  th game\n",
            "Playing  669  th game\n",
            "Playing  670  th game\n",
            "Playing  671  th game\n",
            "Playing  672  th game\n",
            "Playing  673  th game\n",
            "Playing  674  th game\n",
            "Playing  675  th game\n",
            "Playing  676  th game\n",
            "Playing  677  th game\n",
            "Playing  678  th game\n",
            "Playing  679  th game\n",
            "Playing  680  th game\n",
            "Playing  681  th game\n",
            "Playing  682  th game\n",
            "Playing  683  th game\n",
            "Playing  684  th game\n",
            "Playing  685  th game\n",
            "Playing  686  th game\n",
            "Playing  687  th game\n",
            "Playing  688  th game\n",
            "Playing  689  th game\n",
            "Playing  690  th game\n",
            "Playing  691  th game\n",
            "Playing  692  th game\n",
            "Playing  693  th game\n",
            "Playing  694  th game\n",
            "Playing  695  th game\n",
            "Playing  696  th game\n",
            "Playing  697  th game\n",
            "Playing  698  th game\n",
            "Playing  699  th game\n",
            "Playing  700  th game\n",
            "Playing  701  th game\n",
            "Playing  702  th game\n",
            "Playing  703  th game\n",
            "Playing  704  th game\n",
            "Playing  705  th game\n",
            "Playing  706  th game\n",
            "Playing  707  th game\n",
            "Playing  708  th game\n",
            "Playing  709  th game\n",
            "Playing  710  th game\n",
            "Playing  711  th game\n",
            "Playing  712  th game\n",
            "Playing  713  th game\n",
            "Playing  714  th game\n",
            "Playing  715  th game\n",
            "Playing  716  th game\n",
            "Playing  717  th game\n",
            "Playing  718  th game\n",
            "Playing  719  th game\n",
            "Playing  720  th game\n",
            "Playing  721  th game\n",
            "Playing  722  th game\n",
            "Playing  723  th game\n",
            "Playing  724  th game\n",
            "Playing  725  th game\n",
            "Playing  726  th game\n",
            "Playing  727  th game\n",
            "Playing  728  th game\n",
            "Playing  729  th game\n",
            "Playing  730  th game\n",
            "Playing  731  th game\n",
            "Playing  732  th game\n",
            "Playing  733  th game\n",
            "Playing  734  th game\n",
            "Playing  735  th game\n",
            "Playing  736  th game\n",
            "Playing  737  th game\n",
            "Playing  738  th game\n",
            "Playing  739  th game\n",
            "Playing  740  th game\n",
            "Playing  741  th game\n",
            "Playing  742  th game\n",
            "Playing  743  th game\n",
            "Playing  744  th game\n",
            "Playing  745  th game\n",
            "Playing  746  th game\n",
            "Playing  747  th game\n",
            "Playing  748  th game\n",
            "Playing  749  th game\n",
            "Playing  750  th game\n",
            "Playing  751  th game\n",
            "Playing  752  th game\n",
            "Playing  753  th game\n",
            "Playing  754  th game\n",
            "Playing  755  th game\n",
            "Playing  756  th game\n",
            "Playing  757  th game\n",
            "Playing  758  th game\n",
            "Playing  759  th game\n",
            "Playing  760  th game\n",
            "Playing  761  th game\n",
            "Playing  762  th game\n",
            "Playing  763  th game\n",
            "Playing  764  th game\n",
            "Playing  765  th game\n",
            "Playing  766  th game\n",
            "Playing  767  th game\n",
            "Playing  768  th game\n",
            "Playing  769  th game\n",
            "Playing  770  th game\n",
            "Playing  771  th game\n",
            "Playing  772  th game\n",
            "Playing  773  th game\n",
            "Playing  774  th game\n",
            "Playing  775  th game\n",
            "Playing  776  th game\n",
            "Playing  777  th game\n",
            "Playing  778  th game\n",
            "Playing  779  th game\n",
            "Playing  780  th game\n",
            "Playing  781  th game\n",
            "Playing  782  th game\n",
            "Playing  783  th game\n",
            "Playing  784  th game\n",
            "Playing  785  th game\n",
            "Playing  786  th game\n",
            "Playing  787  th game\n",
            "Playing  788  th game\n",
            "Playing  789  th game\n",
            "Playing  790  th game\n",
            "Playing  791  th game\n",
            "Playing  792  th game\n",
            "Playing  793  th game\n",
            "Playing  794  th game\n",
            "Playing  795  th game\n",
            "Playing  796  th game\n",
            "Playing  797  th game\n",
            "Playing  798  th game\n",
            "Playing  799  th game\n",
            "Playing  800  th game\n",
            "Playing  801  th game\n",
            "Playing  802  th game\n",
            "Playing  803  th game\n",
            "Playing  804  th game\n",
            "Playing  805  th game\n",
            "Playing  806  th game\n",
            "Playing  807  th game\n",
            "Playing  808  th game\n",
            "Playing  809  th game\n",
            "Playing  810  th game\n",
            "Playing  811  th game\n",
            "Playing  812  th game\n",
            "Playing  813  th game\n",
            "Playing  814  th game\n",
            "Playing  815  th game\n",
            "Playing  816  th game\n",
            "Playing  817  th game\n",
            "Playing  818  th game\n",
            "Playing  819  th game\n",
            "Playing  820  th game\n",
            "Playing  821  th game\n",
            "Playing  822  th game\n",
            "Playing  823  th game\n",
            "Playing  824  th game\n",
            "Playing  825  th game\n",
            "Playing  826  th game\n",
            "Playing  827  th game\n",
            "Playing  828  th game\n",
            "Playing  829  th game\n",
            "Playing  830  th game\n",
            "Playing  831  th game\n",
            "Playing  832  th game\n",
            "Playing  833  th game\n",
            "Playing  834  th game\n",
            "Playing  835  th game\n",
            "Playing  836  th game\n",
            "Playing  837  th game\n",
            "Playing  838  th game\n",
            "Playing  839  th game\n",
            "Playing  840  th game\n",
            "Playing  841  th game\n",
            "Playing  842  th game\n",
            "Playing  843  th game\n",
            "Playing  844  th game\n",
            "Playing  845  th game\n",
            "Playing  846  th game\n",
            "Playing  847  th game\n",
            "Playing  848  th game\n",
            "Playing  849  th game\n",
            "Playing  850  th game\n",
            "Playing  851  th game\n",
            "Playing  852  th game\n",
            "Playing  853  th game\n",
            "Playing  854  th game\n",
            "Playing  855  th game\n",
            "Playing  856  th game\n",
            "Playing  857  th game\n",
            "Playing  858  th game\n",
            "Playing  859  th game\n",
            "Playing  860  th game\n",
            "Playing  861  th game\n",
            "Playing  862  th game\n",
            "Playing  863  th game\n",
            "Playing  864  th game\n",
            "Playing  865  th game\n",
            "Playing  866  th game\n",
            "Playing  867  th game\n",
            "Playing  868  th game\n",
            "Playing  869  th game\n",
            "Playing  870  th game\n",
            "Playing  871  th game\n",
            "Playing  872  th game\n",
            "Playing  873  th game\n",
            "Playing  874  th game\n",
            "Playing  875  th game\n",
            "Playing  876  th game\n",
            "Playing  877  th game\n",
            "Playing  878  th game\n",
            "Playing  879  th game\n",
            "Playing  880  th game\n",
            "Playing  881  th game\n",
            "Playing  882  th game\n",
            "Playing  883  th game\n",
            "Playing  884  th game\n",
            "Playing  885  th game\n",
            "Playing  886  th game\n",
            "Playing  887  th game\n",
            "Playing  888  th game\n",
            "Playing  889  th game\n",
            "Playing  890  th game\n",
            "Playing  891  th game\n",
            "Playing  892  th game\n",
            "Playing  893  th game\n",
            "Playing  894  th game\n",
            "Playing  895  th game\n",
            "Playing  896  th game\n",
            "Playing  897  th game\n",
            "Playing  898  th game\n",
            "Playing  899  th game\n",
            "Playing  900  th game\n",
            "Playing  901  th game\n",
            "Playing  902  th game\n",
            "Playing  903  th game\n",
            "Playing  904  th game\n",
            "Playing  905  th game\n",
            "Playing  906  th game\n",
            "Playing  907  th game\n",
            "Playing  908  th game\n",
            "Playing  909  th game\n",
            "Playing  910  th game\n",
            "Playing  911  th game\n",
            "Playing  912  th game\n",
            "Playing  913  th game\n",
            "Playing  914  th game\n",
            "Playing  915  th game\n",
            "Playing  916  th game\n",
            "Playing  917  th game\n",
            "Playing  918  th game\n",
            "Playing  919  th game\n",
            "Playing  920  th game\n",
            "Playing  921  th game\n",
            "Playing  922  th game\n",
            "Playing  923  th game\n",
            "Playing  924  th game\n",
            "Playing  925  th game\n",
            "Playing  926  th game\n",
            "Playing  927  th game\n",
            "Playing  928  th game\n",
            "Playing  929  th game\n",
            "Playing  930  th game\n",
            "Playing  931  th game\n",
            "Playing  932  th game\n",
            "Playing  933  th game\n",
            "Playing  934  th game\n",
            "Playing  935  th game\n",
            "Playing  936  th game\n",
            "Playing  937  th game\n",
            "Playing  938  th game\n",
            "Playing  939  th game\n",
            "Playing  940  th game\n",
            "Playing  941  th game\n",
            "Playing  942  th game\n",
            "Playing  943  th game\n",
            "Playing  944  th game\n",
            "Playing  945  th game\n",
            "Playing  946  th game\n",
            "Playing  947  th game\n",
            "Playing  948  th game\n",
            "Playing  949  th game\n",
            "Playing  950  th game\n",
            "Playing  951  th game\n",
            "Playing  952  th game\n",
            "Playing  953  th game\n",
            "Playing  954  th game\n",
            "Playing  955  th game\n",
            "Playing  956  th game\n",
            "Playing  957  th game\n",
            "Playing  958  th game\n",
            "Playing  959  th game\n",
            "Playing  960  th game\n",
            "Playing  961  th game\n",
            "Playing  962  th game\n",
            "Playing  963  th game\n",
            "Playing  964  th game\n",
            "Playing  965  th game\n",
            "Playing  966  th game\n",
            "Playing  967  th game\n",
            "Playing  968  th game\n",
            "Playing  969  th game\n",
            "Playing  970  th game\n",
            "Playing  971  th game\n",
            "Playing  972  th game\n",
            "Playing  973  th game\n",
            "Playing  974  th game\n",
            "Playing  975  th game\n",
            "Playing  976  th game\n",
            "Playing  977  th game\n",
            "Playing  978  th game\n",
            "Playing  979  th game\n",
            "Playing  980  th game\n",
            "Playing  981  th game\n",
            "Playing  982  th game\n",
            "Playing  983  th game\n",
            "Playing  984  th game\n",
            "Playing  985  th game\n",
            "Playing  986  th game\n",
            "Playing  987  th game\n",
            "Playing  988  th game\n",
            "Playing  989  th game\n",
            "Playing  990  th game\n",
            "Playing  991  th game\n",
            "Playing  992  th game\n",
            "Playing  993  th game\n",
            "Playing  994  th game\n",
            "Playing  995  th game\n",
            "Playing  996  th game\n",
            "Playing  997  th game\n",
            "Playing  998  th game\n"
          ]
        },
        {
          "ename": "HangmanAPIError",
          "evalue": "{'error': 'You have reached 1000 of games', 'status': 'denied'}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHangmanAPIError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[89], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlaying \u001b[39m\u001b[38;5;124m'\u001b[39m, i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m th game\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Uncomment the following line to execute your final runs. Do not do this until you are satisfied with your submission\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpractice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# DO NOT REMOVE as otherwise the server may lock you out for too high frequency of requests\u001b[39;00m\n\u001b[1;32m      7\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.5\u001b[39m)\n",
            "Cell \u001b[0;32mIn[66], line 89\u001b[0m, in \u001b[0;36mHangmanAPI.start_game\u001b[0;34m(self, practice, verbose)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmisses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_dictionary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_dictionary\n\u001b[0;32m---> 89\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/new_game\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpractice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mpractice\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapproved\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     game_id \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgame_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[0;32mIn[66], line 191\u001b[0m, in \u001b[0;36mHangmanAPI.request\u001b[0;34m(self, path, args, post_args, method)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HangmanAPIError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaintype was not text, or querystring\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HangmanAPIError(result)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[0;31mHangmanAPIError\u001b[0m: {'error': 'You have reached 1000 of games', 'status': 'denied'}"
          ]
        }
      ],
      "source": [
        "for i in range(1000):\n",
        "    print('Playing ', i, ' th game')\n",
        "    # Uncomment the following line to execute your final runs. Do not do this until you are satisfied with your submission\n",
        "    api.start_game(practice=0,verbose=False)\n",
        "    \n",
        "    # DO NOT REMOVE as otherwise the server may lock you out for too high frequency of requests\n",
        "    time.sleep(0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "overall success rate = 0.619\n"
          ]
        }
      ],
      "source": [
        "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
        "success_rate = total_recorded_successes/total_recorded_runs\n",
        "print('overall success rate = %.3f' % success_rate)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
